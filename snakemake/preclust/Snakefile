"""
    snakemake --keep-going -j 999999 --cluster "sbatch --exclude={cluster.exclude} -c {cluster.ntasks} -N {cluster.Nodes}  -t {cluster.runtime} -J {cluster.jobname} --mail-type={cluster.mail_type} --mail-user={cluster.mail}" --cluster-config cluster.json --configfile experimental_experiments.json --latency-wait 100 --verbose -n
    snakemake  --configfile experimental_experiments.json --latency-wait 100 --verbose -n
    snakemake --configfile experimental_experiments.json --latency-wait 100 --verbose --rerun-incomplete quality -n 
    snakemake --rulegraph --configfile experiments.json | dot -Tpng > figures/ruledag.png


    # preprocessing
    1. bax to bam
    2. subreads to ccs, to merged ccs
    3. ccs to demultiplexed ccs w/o barcodes (lima)

    # simulation
    4. Simlord
    5. adding of tags with "add_read_tags_to_bam.py"
    
    # algorithms
    3. demultiplexed ccs to isoseq3 cluster
    3'. (1) demultiplexed ccs (w. q values) and (2) the accessions of the flnc ccs reads produced by isoseq3 cluster to qt-clust (check if seqs have been altered!)

    # evaluation
    4. alignment of reads to hg38 (skip this if simulated data)
    5. Fix cluster-files produced by cluster and qt-clust to a unified format
    6. Run "compute_cluster_quality.py" and get various metrics

    # structure
    indata: 
    root folder: /nfs/brubeck.bx.psu.edu/scratch6/ksahlin/preclust_eval/
    bam_folder, ccs_folder, lima_folder, 
    isoseq3_folder, simlord_folder
    flnc_fastq_folder


    # target rules:
    preprocess 
    simulation
    cluster
    evaluation

"""

shell.prefix("set -o pipefail; ")
# configfile: "experiments.json"

####################################################
########## standard python functions ###############
####################################################

import re
import os
import errno
import shutil

def mkdir_p(path):
    print("creating", path)
    try:
        os.makedirs(path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

def parse_gnu_time(stderr_file):
    lines = open(stderr_file, 'r').readlines()
    print(lines)
    for l in lines:
        usertime_match =  re.search('User time \(seconds\): [\d.]+', l)
        wct_match = re.search('Elapsed \(wall clock\) time \(h:mm:ss or m:ss\): [\d.:]+', l) 
        mem_match = re.search('Maximum resident set size \(kbytes\): [\d.:]+', l) 
        if usertime_match:
            usertime = float(usertime_match.group().split(':')[1].strip())
        if wct_match:
            wallclocktime = wct_match.group().split()[7]
        if mem_match:
            mem_tmp = int(mem_match.group().split()[5])
            memory_gb = mem_tmp / 4000000.0 

    vals = list(map(lambda x: float(x), wallclocktime.split(":") ))
    if len(vals) == 3:
        h,m,s = vals
        tot_wallclock_secs = h*3600.0 + m*60.0 + s
    elif len(vals) == 2:
        m,s = vals
        tot_wallclock_secs = m*60.0 + s

    return usertime, tot_wallclock_secs, memory_gb


def read_fasta(fasta_file):
    fasta_seqs = {}
    k = 0
    temp = ''
    accession = ''
    for line in fasta_file:
        if line[0] == '>' and k == 0:
            accession = line[1:].strip().split()[0]
            fasta_seqs[accession] = ''
            k += 1
        elif line[0] == '>':
            yield accession, temp
            temp = ''
            accession = line[1:].strip().split()[0]
        else:
            temp += line.strip()
    if accession:
        yield accession, temp


def clean_dir(folder):
    keep_files = set(["final_candidates.fa", "cluster_report.csv", "cluster_summary.txt", "logfile.txt", "final_candidates_lq.fa"]) 
    for the_file in os.listdir(folder):
        if the_file in keep_files:
            continue

        file_path = os.path.join(folder, the_file)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path): 
                shutil.rmtree(file_path)
        except Exception as e:
            print(e)
#######################################



############## TARGET FILES ####################

# PREPROCESS
TARGET_FILES = {}

def subreads_files(datasets):
    files = []
    for dataset in datasets:
        for movie in config[dataset]["movies"]:
            files.append(config["DATA"] + "{0}/subreads/{1}.subreads.bam".format(dataset, movie))
    return files


def ccs_files(datasets):
    files = []
    for dataset in datasets:
        for movie in config[dataset]["movies"]:
            files.append(config["DATA"] + "{0}/ccs/{1}.ccs.bam".format(dataset, movie))
    return files

def lima_files(datasets):
    files = []
    for dataset in datasets:
        for movie in config[dataset]["movies"]:
            files.append(config["DATA"] + "{0}/lima/{1}.demux.primer_5p--primer_3p.bam".format(dataset, movie))
    return files

# subreads_files = lambda dataset: expand(config["DATA"] + "{dataset}/subreads/{movie}.subreads.bam", dataset = dataset, movie = config[dataset]["movies"] )
# ccs_files = lambda wildcards: expand(config["DATA"] + "{dataset}/ccs/{movie}.ccs.bam", dataset = config["DATASET"], movie = config[wildcards.dataset]["movies"] )
# lima_files = lambda wildcards: expand(config["DATA"] + "{dataset}/lima/{movie}.lima.bam", dataset = config["DATASET"], movie = config[wildcards.dataset]["movies"])


TARGET_FILES['preprocess'] = subreads_files(config["DATASET"]) + ccs_files(config["DATASET"]) + lima_files(config["DATASET"])

#CLUSTER

isoseq3_files = expand(config["ROOT_OUT"] + "cluster/isoseq3/{dataset}/unpolished.{suffix}", dataset = config["DATASET"], suffix = ["bam", "cluster", "flnc.bam"]) 
# qt_qlust_files =
TARGET_FILES["cluster"] = isoseq3_files

#EVALUATION REAL

eval_real_files = expand(config["ROOT_OUT"] + "evaluation/{dataset}/{tool}_table.tsv", dataset = config["DATASET"], tool = ["isoseq3"]) 

TARGET_FILES["evaluation"] = eval_real_files #+ eval_sim_files


#EVALUATION SIMULATED



# rule all:
#     input:  TARGET_FILES["preprocess"], TARGET_FILES["simulation"], TARGET_FILES["cluster"], TARGET_FILES["evaluation"]

rule preprocess:
        input: TARGET_FILES["preprocess"]

# rule simulation:
#         input: TARGET_FILES["simulation"]

rule cluster:
        input: TARGET_FILES["cluster"]

rule evaluation:
        input: TARGET_FILES["evaluation"]

#####################################################

rule bax2bam:
        input: #hdf5_files = config["RAW_DATA"] + "{batch_size}/{movie,*bax.h5}"
        output: bam_subreads = config["DATA"] + "{dataset}/subreads/{movie}.subreads.bam",
                bam_index = config["DATA"] + "{dataset}/subreads/{movie}.subreads.bam.pbi"
        run:
            pass
            # shell("bax2bam  $raw_path$movie\_s1_p0.1.bax.h5 $raw_path$movie\_s1_p0.2.bax.h5 $raw_path$movie\_s1_p0.3.bax.h5 -o {wildcards.dataset}/subreads/{wildcards.movie} ")


rule ccs:
    input: bam_subreads = rules.bax2bam.output.bam_subreads
    output: ccs_bam = config["DATA"] + "{dataset}/ccs/{movie}.ccs.bam",
            bam_index = config["DATA"] + "{dataset}/ccs/{movie}.ccs.bam.pbi",
    run:
        shell("ccs --minPasses=1 --numThreads=16 --polish {input.bam_subreads} {output.ccs_bam} ")


rule lima:
    input: ccs_reads = rules.ccs.output.ccs_bam, 
            primers = config["DATA"] + "{dataset}/primers.fasta"
    output: demultiplexed_ccs_reads = config["DATA"] + "{dataset}/lima/{movie}.demux.primer_5p--primer_3p.bam" 
    run:
        shell("source activate isoseq3")
        out = config["DATA"] + "{0}/lima/{1}.demux.bam".format(wildcards.dataset, wildcards.movie)
        shell("lima {input.ccs_reads} {input.primers} {out} --isoseq --no-pbi")


rule merge_demultiplexed_files:
    input: infiles = lambda wildcards: expand(rules.lima.output.demultiplexed_ccs_reads, dataset = wildcards.dataset, movie = config[wildcards.dataset]["movies"]) 
    output: outfile = config["ROOT_OUT"] + "ccs/{dataset}/ccs.bam" 
    
    run:
        shell("samtools merge {output.outfile} {input.infiles}")
        shell("pbindex {output.outfile}")


rule isoseq3:
    input:  ccs = rules.merge_demultiplexed_files.output.outfile
    output: time_and_mem = config["ROOT_OUT"] + "time_and_mem/isoseq3/{dataset}/runtime.stdout",
            clusters = config["ROOT_OUT"] + "cluster/isoseq3/{dataset}/unpolished.cluster",
            flnc = config["ROOT_OUT"] + "cluster/isoseq3/{dataset}/unpolished.flnc.bam",
            consensus = config["ROOT_OUT"] + "cluster/isoseq3/{dataset}/unpolished.bam"
    run:
        shell("source activate isoseq3")
        time = config["GNUTIME"]
        mkdir_p(config["ROOT_OUT"] + "time_and_mem/isoseq3/{0}/".format(wildcards.dataset) )
        mkdir_p(config["ROOT_OUT"] + "cluster/isoseq3/{0}/".format(wildcards.dataset) )
        shell("{time} isoseq3 cluster --num-threads 8 {input.ccs} {output.consensus} --verbose 2>&1 | tee {output.time_and_mem} ")


# rule qt_clust:
#     input:  flnc = rules.isoseq3.output.flnc,
#             ccs  = rules.merge_demultiplexed_files.output.outfile
#     output: time_and_mem = config["ROOT_OUT"] + "time_and_mem/qt_clust/{dataset}/runtime.stdout",
#             clusters = config["ROOT_OUT"] + "cluster/qt_clust/{dataset}/pre_clusters.csv",
#     run:
#         time = config["GNUTIME"]
#         mkdir_p(config["ROOT_OUT"] + "time_and_mem/qt_clust/{0}/".format(wildcards.dataset) )
#         outfolder = config["ROOT_OUT"] + "cluster/qt_clust/{0}/".format(wildcards.dataset)

#         mkdir_p(config["ROOT_OUT"] + "cluster/qt_clust/{0}/".format(wildcards.dataset) )
#         shell("{time} /galaxy/home/ksahlin/prefix/source/isocon_nontargeted/modules/preclust3.py --t 8 --flnc {input.flnc} --ccs {input.ccs}  --outfolder {outfolder}  2>&1 | tee {output.time_and_mem}")


rule minimap2_align:
    input:  reads = rules.isoseq3.output.flnc,
            ref = config["MMI"]
    output: fastq = "/nfs/brubeck.bx.psu.edu/scratch6/ksahlin/preclust_eval/read_alignment/{dataset}.fastq"
            alignment = "/nfs/brubeck.bx.psu.edu/scratch6/ksahlin/preclust_eval/read_alignment/{dataset}.sam"
    
    run:
        shell("bamtools convert -format fastq -in {input.reads} -out {output.fastq} ")
        shell("/usr/bin/time -v  minimap2 -t 8 -ax splice -uf -C5 {input.ref} {output.fastq} >  {output.alignment} ")


rule file_convert_isoseq3_clusters:
    input: clusters = rules.isoseq3.output.clusters
    output: clusters = config["ROOT_OUT"] + "cluster/isoseq3/{dataset}/unpolished.mod_format.cluster",
    run:
        script_path = config["SCRIPT_FOLDER"]
        shell("python {script_path}/modify_cluster_format.py {input.clusters} {output.clusters} ")

# rule file_convert_qt_clust_clusters:

rule v_measure:
    input: 
        true  = rules.minimap2_align.output.alignment,
        isoseq3_predicted  = rules.file_convert_isoseq3_clusters.output.clusters,
        # qt_clust_predicted = rules.qt_clust.output.clusters
    output: 
        isoseq3_res = config["ROOT_OUT"] + "evaluation/{dataset}/isoseq3_table.tsv"
        # qt_clust_res = config["ROOT_OUT"] + "evaluation/{dataset}/qt_clust.tsv"

    run:
        script_path = config["SCRIPT_FOLDER"]
        mkdir_p(config["ROOT_OUT"] + "evaluation/{0}/".format(wildcards.dataset))
        shell("python {script_path}/compute_cluster_quality.py --clusters {input.isoseq3_predicted} --classes {input.true}  --outfile {output.isoseq3_res}")
        # shell("python compute_cluster_quality.py --clusters {input.isoseq3_predicted} --classes {input.true}  --outfolder {output.qt_clust_res}")


# rule minimap2:
#     input: flnc = config["ROOT_OUT"] + "/targeted/{batch_size}_{polish}/{primer_id}/flnc.fasta"
#     output: flnc_aln = config["ROOT_OUT"] + "/Illumina/illumina_alignments/{EXPERIMENT_ID}/FLNC/{batch_size}_{polish}/{primer_id}/flnc_rmdup.bam",
#     run:
#         sample_id, gene = primer_to_family_and_sample[wildcards.batch_size][int(wildcards.primer_id)]
#         illumina_reads1 = config["ROOT_OUT"] + "/Illumina/{0}_{1}_L001_R1_001.fastq".format(gene, sample_id)
#         illumina_reads2 = config["ROOT_OUT"] + "/Illumina/{0}_{1}_L001_R2_001.fastq".format(gene, sample_id)

#         unique_sequences = { seq : acc for (acc, seq) in  read_fasta(open(input.flnc, 'r'))}

#         ################
#         tmp_folder = "/galaxy/home/ksahlin/prefix/tmp/tmp_BWA_{0}_targeted_{1}_{2}_{3}_{4}".format(wildcards.EXPERIMENT_ID, wildcards.batch_size, wildcards.polish, wildcards.primer_id, "FLNC")
#         mkdir_p(tmp_folder)
        
#         batch_index = 0
#         tmp_file = os.path.join(tmp_folder, "unique_candidates_{0}.fa".format(batch_index))
#         unique_seqs = open(tmp_file, "w")

#         for i, (seq, acc) in enumerate(unique_sequences.items()):
#             if i % 50 == 0 and i > 0:
#                 unique_seqs.close()
#                 batch_index += 1
#                 tmp_file = os.path.join(tmp_folder, "unique_candidates_{0}.fa".format(batch_index))
#                 unique_seqs = open(tmp_file, "w")
#                 unique_seqs.write(">{0}\n{1}\n".format(acc,seq))

#             else:
#                 unique_seqs.write(">{0}\n{1}\n".format(acc,seq))

#         unique_seqs.close()
#         # unique_tmp_file_name = unique_seqs.name
#         fofn_bam = os.path.join( config["ROOT_OUT"] + "/Illumina/illumina_alignments/{0}/{1}/{2}_{3}/{4}/bamlist.fofn".format(wildcards.EXPERIMENT_ID, "FLNC", wildcards.batch_size, wildcards.polish, wildcards.primer_id))
#         shell("> {fofn_bam}")
#         for i in range(batch_index +1):         
#             unique_tmp_file_name =  os.path.join(tmp_folder, "unique_candidates_{0}.fa".format(i))
#             base_name_file = config["ROOT_OUT"] + "/Illumina/illumina_alignments/{0}/{1}/{2}_{3}/{4}/hq_{5}".format(wildcards.EXPERIMENT_ID, "FLNC", wildcards.batch_size, wildcards.polish, wildcards.primer_id, i)
#             shell("rm -f {base_name_file}*")
#             shell("bwa index {unique_tmp_file_name}")
#             shell("bwa mem -t 8 {unique_tmp_file_name} {illumina_reads1} {illumina_reads2} > {base_name_file}.sam")
#             shell("samtools view -b {base_name_file}.sam > {base_name_file}.bam")
#             shell("samtools sort {base_name_file}.bam -o {base_name_file}_sorted.bam")
#             shell("samtools rmdup {base_name_file}_sorted.bam {base_name_file}_rmdup.bam")
#             # shell("samtools index {base_name_file}_rmdup.bam")
#             shell("echo {base_name_file}_rmdup.bam >> {fofn_bam}")
        
#         shell("samtools merge -f -b {fofn_bam}  {output.flnc_aln}")
#         shell("samtools index {output.flnc_aln}")
#         ##################


# rule format_clusters_isoseq3:
# rule format_clusters_preclust:



# rule compute_clustering_metrics:
#     input: flnc = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/FLNC/{polish}_isoform_hits_{database}_hq.tsv", # rules.isoform_align_flnc.output.flnc_tsv,
#             isocon_hq = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/ISOCON/{polish}_isoform_hits_{database}_hq.tsv",
#             # isocon_lq = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/ISOCON/{polish}_isoform_hits_{database}_lq.tsv",
#             ice_hq = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/ICE_QUAL/unpolished_isoform_hits_{database}_hq.tsv",
#             # ice_hq = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/ICE_QUAL/{polish}_isoform_hits_{database}_hq.tsv",
#             # ice_lq = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/ICE_QUAL/{polish}_isoform_hits_{database}_lq.tsv"
#             proovread = config["ROOT"]+"results/{EXPERIMENT_ID}/targeted/PROOVREAD/{polish}_isoform_hits_{database}_hq.tsv"
#     output: isoforms_in_database_hq_plot = config["ROOT_OUT"]+"/results/{EXPERIMENT_ID}/targeted/{polish}_detected_in_{database}_hq.png",
#             # isoforms_in_database_hq_and_lq_plot = config["ROOT_OUT"]+"/results/{EXPERIMENT_ID}/targeted/{polish}_detected_in_{database}_lq_and_hq.png",
#             venn_diagram_hq = config["ROOT_OUT"]+"/results/{EXPERIMENT_ID}/targeted/{polish}_detected_in_{database}_hq_venn.png",
#             # venn_diagram_hq_and_lq = config["ROOT_OUT"]+"/results/{EXPERIMENT_ID}/targeted/{polish}_detected_in_{database}_lq_and_hq_venn.png"
#     run:
#         # python ../analysis/isoform_level/plot_isoform_hits.py --flnc ~/tmp/ISOFORM_ANALYSIS/07_22_17_CCS_POLISHED/pred_to_ref_best_alignments.tsv  --isocon ~/tmp/ISOFORM_ANALYSIS/07_22_17_ISOCON_polished_ccs/pred_to_ref_best_alignments.tsv --ice ~/tmp/ISOFORM_ANALYSIS/07_22_17_ICE_Q_polished_ccs/pred_to_ref_best_alignments.tsv  --outfolder ~/tmp/ISOFORM_ANALYSIS/results/identity_temp
#         # python perfect_matches_set_intersection.py ~/tmp/ISOFORM_ANALYSIS/results_all_high_qual_and_low_qual/identity_only_perfect_matches/hit_to_db.tsv  ~/tmp/ISOFORM_ANALYSIS/results_all_high_qual_and_low_qual/identity_only_perfect_matches/venn_diagram
        
#         script_folder = config["SCRIPT_FOLDER"]
#         db_transcripts = config[wildcards.database]
#         shell("python {script_folder}/plot_isoform_hits.py --flnc {input.flnc} --isocon {input.isocon_hq} --proovread {input.proovread} --ice {input.ice_hq} --database {db_transcripts} --outprefix {output.isoforms_in_database_hq_plot} ")
#         # shell("python {script_folder}/plot_isoform_hits.py --flnc {input.flnc} --isocon {input.isocon_hq} {input.isocon_lq} --ice {input.ice_hq} {input.ice_lq} --outprefix {output.isoforms_in_database_hq_and_lq_plot} ")
#         shell("python {script_folder}/perfect_matches_set_intersection.py {output.isoforms_in_database_hq_plot}_hit_to_db.tsv {output.venn_diagram_hq}")
#         # shell("python {script_folder}/perfect_matches_set_intersection.py {output.isoforms_in_database_hq_and_lq_plot}_hit_to_db.tsv {output.venn_diagram_hq_and_lq}")


# rule plot_number_passes:
#     input: ccs_bam = rules.ccs.output.ccs_bam
#     output: nr_passes_plot = config["ROOT"]+"results/{EXPERIMENT_ID}/{experiment}/{batch_size}_{polish}/nr_passes_plot.pdf"
#     run:
#         script_folder = config["SCRIPT_FOLDER"]
#         shell("python3 {script_folder}/nr_passes_plot.py  --bamfile {input.ccs_bam}  --outfile {output.nr_passes_plot}")
#     # python nr_passes_plot.py --bamfile ~/tmp/CCSTune/barcode_1-2kb_polished.ccs.bam --outfile ~/tmp/tpm_passes.pdf


